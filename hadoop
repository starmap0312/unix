# hadoop fs <args>
  1) list a folder (option -ls -R: list recursively)
     ex. hadoop fs -ls /path/to/hadoop/folder
         hadoop fs -ls /path/to/hadoop/folder
     ex. hadoop fs -ls -R /path/to/hadoop/folder
     ex. hadoop fs -ls hdfs://[cluster_url]:8020/path/to/hadoop/folder (list hdfs folder via the full hdfs path) 
  2) download a hadoop folder
     ex. hadoop fs -get /path/to/folder
  3) upload a local folder / file to hadoop
     ex. hadoop fs -put /path/to/local/folder/ /path/to/hadoop/folder
         hadoop fs -put -f /path/to/local/folder/filename /path/to/hadoop/folder/filename
           -f: force, to overwirte
  4) cat a hadoop file
     ex. hadoop fs -cat /path/to/filename
  5) copy a hadoop file
     options:
       -f: overwrite the destination if it already exists
     ex. hadoop fs -cp /path/to/file1 /path/to/file2
  5) count the number of directories, files and bytes under the path
     ex. hadoop fs -count -q /path/to/hadoop/folder
         hdfs dfs -count -q hdfs://nn1.example.com/file1
  6) make a folder
     hadoop fs -mkdir [-p] /path/to/folder
  7) move a folder
     hadoop fs -mv /user/hadoop/folder1 /user/hadoop/folder2
  8) remove a folder recursively
     hadoop fs -rm -r /user/hadoop/folder
     hadoop fs -rm -r -skipTrash /user/hadoop/folder  (do not move the files to /user/hadoop/.Trash)
  9) hadoop dfs -count <paths>
     ex. hdfs dfs -count hdfs://nn1.example.com/file1 hdfs://nn2.example.com/file2
 10) outputs the file in text format (allowed formats: zip and TextRecordInputStream)
     ex. hadoop fs -text /path/to/filename.zip
         hadoop fs -text /path/to/filename.tgz
         hadoop fs -text /path/to/000000_0.deflate (to decompress a hadoop deflate file in shell, use "openssl zlib -d -in 004999_1.deflate")
     hadoop fs -text vs. hadoop fs -cat
       hadoop fs -text detects the encoding of the file and decodes it to plain text whenever possible
       hadoop fs -cat does not try to detect the encoding and do the decoding

# hadoop fsck: runs a HDFS filesystem checking utility
  hadoop fsck <path>: start checking from this path
  ex. hadoop fsck /

# hadoop distcp: copy data between two clusters
  hadoop distcp hftp://cdh3-namenode:50070/ hdfs://cdh4-nameservice/
  options:
    -update   : overwrite if src size different from dst size
    -overwrite: overwrite destination
  ex.
    hadoop distcp -update hftp://cdh3-namenode:50070/ hdfs://cdh4-nameservice/

# Hadoop Permissions
# 1) traditional POSIX permissions model
# 1.1) hadoop fs -chown [-R] [OWNER][:[GROUP]] URI [URI ]
  change the owner of files (file owner and file group)
  ex.
    hadoop fs -chown user:hadoop /path/to/file

# 1.2) hadoop fs -chmod [-R] <MODE[,MODE]... | OCTALMODE> URI [URI ...]
  change the permissions of files
  ex.
    hadoop fs -chmod 755 /path/to/file

# 2) POSIX ACLs (Access Control Lists)
  useful for implementing permission requirements that differ from the natural organizational hierarchy of users and groups
  it provides a way to set different permissions for specific named users or named groups, not only file owner and file group
  an ACL consists of a set of ACL entries:
    each ACL entry names a specific user/group and grants/denies read/write/execute permissions for that specific user/group
    ex.
      user :       :rw-               #               (unnamed user , i.e. file owner, has read-write access)
      group:       :r-x               # effective:r-- (unnamed group, i.e. file group, has read-execute access)
      other:       :r--               #               (unnamed other,                , has read access)
      user :starmap:rwx               # effective:r-- (named user   , i.e. starmap   , has full access)
      group:yahoo  :rwx               # effective:r-- (named group  , i.e. yahoo     , has full access)
      mask :       :r--               #               (mask,        ,                , filters permissions of all named user and named group)
    the algorithm for permission checks:
      file owner -> named user (masked) -> file group (masked) -> named group (masked) -> other permissions
# 2.1) hadoop fs -getfacl [-R] <path>
  dsplay the Access Control Lists (ACLs) of files and directories
  option:
    -R: List the ACLs of all files and directories recursively
# 2.2) hadoop fs -setfacl [-R] [-b |-k -m |-x <acl_spec> <path>]
  set Access Control Lists (ACLs) of files and directories
  option:
    -m: Modify ACL. New entries are added to the ACL, and existing entries are retained
  ex.
    hadoop fs -setfacl -m user:hadoop:rw- /path/to/file

# run hadoop mapreduce jobs
1) run python script command
$HADOOP_PREFIX/bin/hadoop jar $HADOOP_PREFIX/share/hadoop/tools/lib/hadoop-streaming.jar\
  -Dmapred.child.env="LD_LIBRARY_PATH=$LD_LIBRARY_PATH:."\
  -Dmapred.child.java.opts="-Xmx10000m"\
  -Dmapreduce.job.queuename=default\
  -Dmapreduce.job.reduces=1\
  -Dmapreduce.task.timeout=6000000\
  -Dmapreduce.map.speculative=true\
  -Dmapreduce.map.memory.mb=40960\
  -Dmapreduce.reduce.memory.mb=40960\
  -Dmapred.map.tasks=1000\
  -files "mapper.py", "reducer.py", "utils.py", "meta.json"\
  -input "hdfs://hadoop.gridserver.com:8020/path/to/hdfs/input_folder/*"\
  -output "/path/to/hdfs/output_folder/"\ # the output folder cannot exist (i.e. execute before submission: hadoop fs rm -r)
  -mapper "python mapper.py"\
  -reducer "python reducer.py"
2) run java main class command:
$HADOOP_PREFIX/bin/hadoop jar $HADOOP_PREFIX/share/hadoop/tools/lib/hadoop-streaming.jar\
  -Dmapred.child.env="LD_LIBRARY_PATH=$LD_LIBRARY_PATH:."\
  -Dmapred.child.java.opts="-Xmx10000m"\
  -Dmapreduce.job.queuename=default\
  -Dmapreduce.job.reduces=0\
  -Dmapreduce.task.timeout=6000000\
  -Dmapreduce.map.speculative=true\
  -Dmapreduce.map.memory.mb=40960\
  -Dmapred.map.tasks=1000\
  -libjars ./mylib-5.54-SNAPSHOT-jar-with-dependencies.jar\
  -files "mylib-5.54-SNAPSHOT-jar-with-dependencies.jar"\
  -input "/path/to/hdfs/input_folder/*"\
  -output "/path/to/hdfs/output_folder"\
  -mapper "java -cp ./mylib-5.54-SNAPSHOT-jar-with-dependencies.jar com.mydomain.mypackage.MyClass" # run MyClass's main()
  (to debug locally)
  htxt /path/to/hdfs/inputfile.gz | java -cp ./mylib-5.54-SNAPSHOT-jar-with-dependencies.jar com.mydomain.mypackage.MyClass

# debug logs
1) check mapreduce job error log
   yarn logs -applicationId application_[app_id]
   ex. yarn logs -applicationId application_1496686551678_490456
2) kill a mapreduce job
   yarn application -kill application_[app_id]
   ex. yarn application -kill application_1496686551678_502502

# hadoop exceptions
(hadoop streaming jobs)
1) Error: java.io.IOException: Broken pipe
     at java.io.FileOutputStream.writeBytes(Native Method)
   ex. the streaming process (your script) is terminating prematurely
       this may be due to the input is considered complete (ex. interpreting an EOF) or a swallowed exception
       either way, Hadoop is trying to feed into via STDIN to your script, but since the application has terminated
       thus STDIN is no longer a valid File Descriptor, and we get a BrokenPipe error
   solution: add stderr traces in your script to see what line of input is causing the problem (exception)

# hadoop command alias
alias htxt='hadoop fs -text'
alias hcat='hadoop fs -cat'
alias hls='hadoop fs -ls'                       # list hdfs folder
alias hlsr='hadoop fs -ls -R'                   # list hdfs folder recursively
alias hrm='hadoop fs -rm'                       # remove hdfs file
alias hrmr='hadoop fs -rm -r'                   # remove hdfs folder
alias hcp='hadoop fs -cp'                       # copy hdfs file to hdfs
alias hmv='hadoop fs -mv'                       # move hdfs file to hdfs
alias hget='hadoop fs -get'                     # copy hdfs file to local
alias hput='hadoop fs -put'                     # copy local file to hdfs
alias hcopyToLocal='hadoop fs -copyToLocal'     # copy hdfs folder to local
alias hmoveToLocal='hadoop fs -moveToLocal'     # move hdfs folder to local
alias hcopyFromLocal='hadoop fs -copyFromLocal' # copy local folder to hdfs
alias hmoveFromLocal='hadoop fs -moveFromLocal' # move local folder to hdfs
alias hmkdir='hadoop fs -mkdir'                 # make hdfs folder: ex. hadoop fs -mkdir -p /user/hadoop/folder
alias hcount='hadoop fs -count'                 # list space usage: ex. hadoop fs -count -q /user/hadoop/*
alias hgetmerge='hadoop fs -getmerge'           # concatenate multiple files: ex. hadoop fs -getmerge /path/to/hdfs/folder1 /path/to/hdfs/folder2 filename.txt
alias hdu='hadoop fs -du'                       # disk usage, ex. hdu -s -h /user/hadoop/folder
alias hchmod='hadoop fs -chmod'                 # change file permissions: ex. hadoop fs -chmod -R 755 /user/hadoop/folder
alias hchown='hadoop fs -chown'

alias hdus='hdfs dfs -dus'
alias hexpunge='hdfs dfs -expunge'
alias hsetrep='hdfs dfs -setrep'
alias htouchz='hdfs dfs -touchz'
alias htest='hdfs dfs -test'
alias hstat='hdfs dfs -stat'
alias htail='hdfs dfs -tail'
alias hchgrp='hdfs dfs -chgrp'
alias hhelp='hdfs dfs -help'

alias shmkdir='sudo -u hdfs hdfs dfs -mkdir'
alias shrmr='sudo -u hdfs hdfs dfs -rmr'
alias shchmod='sudo -u hdfs hdfs dfs -chmod'
alias shchown='sudo -u hdfs hdfs dfs -chown'
