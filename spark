# Apache Spark vs. Apache Hadoop (Map-Reduce)

# Cluster Mode Overview
1) Spark applications run as independent sets of processes on a cluster, coordinated by the SparkContext object
   in the main program (i.e. the driver program)
2) to run on a cluster, the SparkContext can connect to several types of cluster managers (ex. standalone cluster manager or YARN),
   which allocate resources across applications
3) once connected, Spark acquires executors on nodes in the cluster (i.e. processes that run computations and store data)
4) next, Spark sends your application code (defined by JAR or Python files) to the executors
5) finally, SparkContext sends tasks to the executors to run
                  <----------------------------->  Worker Node (Executor: Task, Task)
   Driver Program        <--> Cluster Master <-->      |
   (SparkContext object)      (ex. YARN)           Worder Node (Executor: Task, Task)
                  <----------------------------->
6) each application gets its own executor processes, which stay up for the duration of the whole application and run tasks in multiple threads
   this isolates applications from each other, on both the scheduling side (each driver schedules its own tasks) and
   executor side (tasks from different applications run in different JVMs)
7) the driver program must listen for and accept incoming connections from its executors throughout its lifetime
   i.e. the driver program must be network addressable from the worker nodes
8) Cluster Manager Types
   Standalone: a simple cluster manager included with Spark that makes it easy to set up a cluster
   Hadoop YARN: the resource manager in Hadoop 2

# dataframe.cache():
  Spark SQL cache tables using an in-memory columnar format by calling spark.catalog.cacheTable("tablename") or dataframe.cache()
  use cache() for DataFrame and tables in the following cases:
  1) reusing them in an iterative loop (ex. ML algos)
  2) reusing the RDD multiple times in a single application, job, or notebook
  3) when the cost to regenerate the RDD partitions is costly (i.e. HDFS, after a complex set of map(), filter(), etc.)
     this helps in the recovery process if a Worker node dies

# yarn-client vs. yarn-cluster mode
1) yarn-client mode:
   the driver runs in the client process, and the AM (application master) is only used for requesting resources from YARN
2) yarn-cluster mode:
   the Spark driver runs inside an AM (application master) process which is managed by YARN on the cluster
   the client can go away after initiating the application

# spark config
1) memory:
   (driver)
   --master yarn --yarn.am.memory 4g: 
     amount of memory to use for the YARN Application Master in client mode
   --master yarn --driver-memory 4g: 
     amount of memory to use for the YARN Application Master in cluster mode
   (executor)
   --conf spark.executor.memory=8g (default: 1g)
     amount of memory to use per executor process
2) initial size of Kryo's serialization buffer:
   --conf spark.kryoserializer.buffer.max=1024 (default: 64m)
     this must be larger than any object you attempt to serialize and must be less than 2048m  
3) jars:
   --conf spark.jars=path/to/file1.jar,path/to/file2.jar
   note: jar files needs to be specified as a relative path of current working folder, and
         they need to be in the same --conf spark.jars=... or --jars ...; otherwise, only the last one will be loaded

# spark-shell & spark-submit commands
1) spark-submit
spark-submit \
--master yarn-client --files /path/to/config_file/hive-site.xml \
--num-executors 100 --conf spark.yarn.stagingDir=/tmp --conf spark.kryoserializer.buffer.max=1024 \
--driver-memory 14g --executor-memory 14g \
--jars ~/.ivy2/local/mydomain/mylib_2.11/0.1.1-SNAPSHOT/jars/mylib_2.11-assembly.jar \
--class MyScalaClass \ # ex. object MyScalaClass { def main(args: Array[String]): Unit = { ... } }
arg1 arg2

2) spark-shell
spark-shell \
--master yarn-client --files /path/to/config_file/hive-site.xml \ 
--num-executors 100 --conf spark.kryoserializer.buffer.max=1024 \
--driver-memory 14g --executor-memory 14g \
--jars ~/.ivy2/local/mydomain/mylib_2.11/0.1.1-SNAPSHOT/jars/mylib_2.11-assembly.jar \
-i MyScalaClass.scala  # ex. println("Hello world") ... line-by-line code in MyScalaClass.scala, no need to define object with main() function 

# Saving DataFrame to Persistent Hive Tables
  DataFrames can also be saved as persistent tables into Hive metastore using the saveAsTable command
  1) Spark will create a default local Hive metastore (using Derby) for you
  2) saveAsTable will materialize the contents of the DataFrame and create a pointer to the data in the Hive metastore
  3) Persistent tables will still exist even after your Spark program has restarted
  4) A DataFrame for a persistent table can be created by calling the table method on a SparkSession with the name of the table
  5) When the table is dropped, the custom table path will not be removed and the table data is still there
     ex. dataframe.write.option("path", "/some/path").saveAsTable("t")
     If no custom table path is specified, Spark will write data to a default table path under the warehouse directory
     When the table is dropped, the default table path will be removed too
     ex. dataframe.write.saveAsTable("t")
  6) Partitioning
     Table partitioning is a common optimization approach used in systems like Hive
     In a partitioned table, data are stored in different directories, with partitioning column values encoded in the path of each partition directory
     ex.
       peopleDF
         .write
         .partitionBy("favorite_color")
         .bucketBy(42, "name")
         .saveAsTable("people_partitioned_bucketed")
     ex. gender and country as partitioning columns
         path
         └── to
             └── table
                 ├── gender=male
                 │   ├── ...
                 │   │
                 │   ├── country=US
                 │   │   └── data.parquet
                 │   ├── country=CN
                 │   │   └── data.parquet
                 │   └── ...
                 └── gender=female
                     ├── ...

# Saving DataFrame
  def write: DataFrameWriter[T]
    interface for saving the content of the non-streaming Dataset out into external storage
    ex. dataframe.repartition(20).write
1) Save as TSV/TextFile: 
   def dataFrameWriter.option(key: String, value: String): DataFrameWriter[T]
     adds an output option for the underlying data source
     ex. dataframe.repartition(20).write.option("sep", "\t")
2) Save as CSV File: 
   def csv(path: String): Unit
     saves the content of the DataFrame in CSV format at the specified path
     ex. dataframe.repartition(20).write.csv("filepath")
3) Save as Parquet File:
   Parquet is a columnar format that is supported by many other data processing systems
   Spark SQL provides support for both reading and writing Parquet files that automatically preserves the schema of the original data
   When writing Parquet files, all columns are automatically converted to be nullable for compatibility reasons
     ex. 
         peopleDF.write.parquet("people.parquet") # saved as Parquet files, maintaining the schema information
         ...
         val parquetFileDF = spark.read.parquet("people.parquet") # Read in the parquet file created above (schema is preserved)

# DataFrame operations
1) def repartition(numPartitions: Int): Dataset[T]
   returns a new DataFrame that has exactly 20 partitions # ex. part-00000, part-00001, ..., part-00020
   ex. dataframe.repartition(20)
5) def from_json(e: Column, schema: StructType): Column
   import org.apache.spark.sql.functions.from_json
   parses a column containing a JSON string into a StructType with the specified schema (returns null, in the case of an unparseable string)

# DataFrame methods
1) def withColumn(colName: String, col: Column): DataFrame 
   returns a new DataFrame by adding a column or replacing the existing column that has the same name
   ex. val schema = StructType(Seq(StructField("key", StringType, true), StructField("value", DoubleType, true)))
       dataframe.withColumn("jsonData", from_json($"jsonStr", schema))
   ex. val toUpperCaseUDF = udf(
         (str: String) => str.toUpperCase
       )
       dataframe.withColumn("UpperCaseColumn", toUpperCaseUDF(col("col1")))
2) def filter(condition: Column): DataFrame
   filters rows using the given condition
   ex. dataframe.filter(col("age") > 15)                              // column age: int
       dataframe.filter(length(col("name")).gt(5))                    // Column name: string
       dataframe.filter(col("name").equalTo("John"))                  // shopping_list
       dataframe.filter(array_contains(col("shopping_list"), "book")) // [tv, book, cd]
   ex. val isHigherUDF = udf(
         (score: Int, threshold: Int) => score > threshold            // (Int, Int) => Boolean
       )
       dataframe.filter(isHigherUDF(col("scores")))

# RDD methods
1) rdd.map([function: Int => Int]) vs. rdd.mapPartitions([function: Iterator[Int] => Iterator[Int]]):
   use of rdd.mapPartitions() to improve the performance
     the map() method maps each element of the rdd to a [function], whereas
     the mapPartitions() method maps each partition's iterator of the rdd to a [function]
   ex. val rdd = sc.parallelize(1 to 9, 3)
       // map():
       def mapDouble(x: Int): (Int, Int) = (x, x * 2)
       val result = rdd.map(mapDouble)
       println(result.collect().mkString) // (1,2)(2,4)(3,6)(4,8)(5,10)(6,12)(7,14)(8,16)(9,18)
   
       // mapPartitions():
       def mapIterDouble(iter: Iterator[Int]): Iterator[(Int, Int)] = {
         var result = List[(Int, Int)]()
         while (iter.hasNext) {
             val x = iter.next
             result .::= (x, x * 2)
         }
         result.iterator
       }
       val result = rdd.mapPartitions(mapIterDouble)
       println(result.collect().mkString) // (1,2)(2,4)(3,6)(4,8)(5,10)(6,12)(7,14)(8,16)(9,18)
2) rdd.groupByKey():
   group the values for each key in the RDD into a single sequence                    //      [key] [value]
   ex. val rdd = sc.makeRDD(Array(("x", 1), ("x", 2), ("y", 1),("y", 2),("z", 1)))    // RDD[(String, Int)]
       rdd.groupByKey                                                                 // RDD[(String, Iterable[Int])]
       rdd.groupByKey.collect  // Array[(String, Iterable[Int])] = Array((z,CompactBuffer(1)), (x,CompactBuffer(1, 2)), (y,CompactBuffer(1, 2)))
3) rdd.groupBy(func: element => key):
   group the values for each element in the RDD by a key function                     //      [key] [value]
   ex. val rdd = sc.makeRDD(Array(("x", 1), ("x", 2), ("y", 1),("y", 2),("z", 1)))    // RDD[(String, Int)]
       rdd.groupBy(e => e._1)                                                         // RDD[(String, Iterable[Int])]
       rdd.groupBy(e => e._1).collect  // Array[(String, Iterable[Int])] = Array((z,CompactBuffer(1)), (x,CompactBuffer(1, 2)), (y,CompactBuffer(1, 2)))
4) rdd.reduceByKey([function: (value1, value2) => merged_value]):
   merge the values for each key using an associative and commutative reduce function //      [key] [value]
   ex. val rdd = sc.makeRDD(Array(("x", 1), ("x", 2), ("y", 1),("y", 2),("z", 1)))    // RDD[(String, Int)]
       rdd.reduceByKey((value1, value2) => value1 + value2)                           // RDD[(String, SumOfInt)] 
       rdd.reduceByKey.collect         // Array[(String, Int)] = Array((z,1), (x,3), (y,3)) 
5) rdd.coalesce([numPartitions: Int]) vs. rdd.repartition([numPartitions: Int]):
   coalesce(); return a new RDD that is reduced into numPartitions partitions
   repartition(): coalesce with performing a shuffle 
     i.e. def repartition(numPartitions: Int) = coalesce(numPartitions, shuffle = true)
   note: if you are decreasing the number of partitions in this RDD, consider using coalesce which can avoid performing a shuffle

# DataFrame methods
1) df.collect():
   return an Array that contains all of Rows in the DataFrame
   ex. val df = sc.makeRDD(Array(("x", 1), ("x", 2), ("y", 1),("y", 2),("z", 1))).toDF("key", "value")
       df.collect                      // Array[Row] = Array([x,1], [x,2], [y,1], [y,2], [z,1])
2) df.groupBy([Column]):
   group the Dataset using the specified columns, so we can run aggregation on them
   it returns class RelationalGroupedDataset, which has a set of methods for aggregations on a DataFrame, ex. count(), max(), min(), avg(), etc.
3) df.groupBy([Column]).count():
   count the number of rows for each group of the DataFrame
   i.e. it maps each group into an integer (count)                                                                    [key]    [value]
   ex. val df = sc.makeRDD(Array(("x", 1), ("x", 2), ("y", 1),("y", 2),("z", 1))).toDF("key", "value") // DataFrame[(string, integer)]
       +---+-----+
       |key|value|
       +---+-----+
       |  x|    1|
       |  x|    2|
       |  y|    1|
       |  y|    2|
       |  z|    1|
       +---+-----+                                                                                                    [key] [count]
       df.groupBy(col("key")).count                                                                    // DataFrame[(string, long)]
       +---+-----+                                                                     
       |key|count|
       +---+-----+
       |  x|    2|
       |  z|    1|
       |  y|    2|
       +---+-----+                                                                                                    [key]  [value] [count]
       df.groupBy(col("key"), col("value")).count                                                      // DataFrame[(string, integer, long)]
       +---+-----+-----+                                                               
       |key|value|count|
       +---+-----+-----+
       |  y|    2|    1|
       |  x|    2|    1|
       |  y|    1|    1|
       |  x|    1|    1|
       |  z|    1|    1|
       +---+-----+-----+
4) df.groupBy([Column]).max([String]): DataFrame
   ex. df.groupBy(col("key")).max("value").show
       +---+----------+
       |key|max(value)|
       +---+----------+
       |  x|         2|
       |  z|         1|
       |  y|         2|
       +---+----------+
       i.e. sqlContext.sql("select key, max(value), from tempTable group By key")
5) df.agg([Column]): returns DataFrame
   ex. val df = sc.makeRDD(Array(("x", 1), ("x", 2), ("y", 1),("y", 2),("z", 1))).toDF("key", "value")
       df.agg(min(col("key")), max(col("value"))).show
       +--------+----------+
       |min(key)|max(value)|
       +--------+----------+
       |       x|         2|
       +--------+----------+

# debug spark
  the --master option specifies the master URL for a distributed cluster, use local mode for debugging
1) spark-shell --master local[N]: run locally with N threads
   ex. spark-shell --master local[4]\
         --num-executors 4\
         --driver-memory 4g\
         --executor-memory 4g\
         --conf spark.jars=local/path/to/mylib1.jar,local/path/to/mylib2.jar\
         --conf spark.authenticate.secret=xxxxxx
         -i MyScript.scala
2) spark-shell --master yarn --deploy-mode client: run as client mode
   --master yarn: deploy Spark on top of Hadoop NextGen (YARN)
     this starts a YARN client program which starts the default Application Master
     the application will be run as a child thread of Application Master
     the client will periodically poll the Application Master for status updates and display them in the console, and will exit once 
       the application has finished running
   --deploy-mode client: launch a Spark application in client mode (default: cluster mode)
3) spark-shell --verbose:
   the --verbose option provides configuration details
4) spark-shell --conf "spark.executor.extraJavaOptions=-verbose:class" --conf "spark.driver.extraJavaOptions=-verbose:class"
   the --verbose:class option reveals the classes loaded by the driver and executor 
   (since Spark application runs on JVM, the --verbose and the --verbose:class options are both available)
5) yarn logs -applicationId [application_id]
   show log file of all containers and AM (Application Master) logs
   ex. yarn logs -applicationId application_1508368153923_55604
6) yarn logs -containerId [container_id]
   show a single container logs
   ex. yarn logs -applicationId application_1508368153923_55368 -containerId container_e33_1480922439133_0845_02_000002

# exceptions
1) org.apache.spark.SparkException: Job aborted due to stage failure: Task 2 in stage 13.0 failed 1 times, most recent failure: Lost task 2.0 in stage 13.0
     Driver stacktrace: at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1564)
   ex. when your script throws exceptions, it causes a job aborted and task lost
2) java.lang.OutOfMemoryError: Java heap space
   ex. when using spark Accumulator/AccumulatorV2, it may produce OutOfMemoryError error
       the accumulator takes too much memory space, so that the driver exhausts its memory space
3) java.lang.OutOfMemoryError: Java heap space + DAGScheduler: Executor lost: 6 (epoch 8)
   ex. when worker exhausts its memory space
4) java.io.IOException: Failed to send RPC 7892527227848920745 to /10.216.141.37:42360: java.nio.channels.ClosedChannelException
   ex. when there is no sufficient disk space (if you are getting this error, make some disk space)

