# Apache Spark vs. Apache Hadoop (Map-Reduce)

# dataframe.cache():
  Spark SQL cache tables using an in-memory columnar format by calling spark.catalog.cacheTable("tablename") or dataframe.cache()
  use cache() for DataFrame and tables in the following cases:
  1) reusing them in an iterative loop (ex. ML algos)
  2) reusing the RDD multiple times in a single application, job, or notebook
  3) when the cost to regenerate the RDD partitions is costly (i.e. HDFS, after a complex set of map(), filter(), etc.)
     this helps in the recovery process if a Worker node dies

# spark-shell & spark-submit commands
1) spark-submit
spark-submit \
--master yarn-client --files /path/to/config_file/hive-site.xml \
--num-executors 100 --conf spark.yarn.stagingDir=/tmp --conf spark.kryoserializer.buffer.max=1024 \
--driver-memory 14g --executor-memory 14g \
--jars ~/.ivy2/local/mydomain/mylib_2.11/0.1.1-SNAPSHOT/jars/mylib_2.11-assembly.jar \
--class MyScalaClass \ # ex. object MyScalaClass { def main(args: Array[String]): Unit = { ... } }
arg1 arg2

2) spark-shell
spark-shell \
--master yarn-client --files /path/to/config_file/hive-site.xml \ 
--num-executors 100 --conf spark.kryoserializer.buffer.max=1024 \
--driver-memory 14g --executor-memory 14g \
--jars ~/.ivy2/local/mydomain/mylib_2.11/0.1.1-SNAPSHOT/jars/mylib_2.11-assembly.jar \
-i MyScalaClass.scala  # ex. println("Hello world") ... line-by-line code in MyScalaClass.scala, no need to define object with main() function 

# Saving DataFrame to Persistent Hive Tables
  DataFrames can also be saved as persistent tables into Hive metastore using the saveAsTable command
  1) Spark will create a default local Hive metastore (using Derby) for you
  2) saveAsTable will materialize the contents of the DataFrame and create a pointer to the data in the Hive metastore
  3) Persistent tables will still exist even after your Spark program has restarted
  4) A DataFrame for a persistent table can be created by calling the table method on a SparkSession with the name of the table
  5) When the table is dropped, the custom table path will not be removed and the table data is still there
     ex. dataframe.write.option("path", "/some/path").saveAsTable("t")
     If no custom table path is specified, Spark will write data to a default table path under the warehouse directory
     When the table is dropped, the default table path will be removed too
     ex. dataframe.write.saveAsTable("t")
  6) Partitioning
     Table partitioning is a common optimization approach used in systems like Hive
     In a partitioned table, data are stored in different directories, with partitioning column values encoded in the path of each partition directory
     ex.
       peopleDF
         .write
         .partitionBy("favorite_color")
         .bucketBy(42, "name")
         .saveAsTable("people_partitioned_bucketed")
     ex. gender and country as partitioning columns
         path
         └── to
             └── table
                 ├── gender=male
                 │   ├── ...
                 │   │
                 │   ├── country=US
                 │   │   └── data.parquet
                 │   ├── country=CN
                 │   │   └── data.parquet
                 │   └── ...
                 └── gender=female
                     ├── ...

# Saving DataFrame to Parquet Files
  1) Parquet is a columnar format that is supported by many other data processing systems
  2) Spark SQL provides support for both reading and writing Parquet files that automatically preserves the schema of the original data
  3) When writing Parquet files, all columns are automatically converted to be nullable for compatibility reasons
     ex. 
         peopleDF.write.parquet("people.parquet") # saved as Parquet files, maintaining the schema information
         ...
         val parquetFileDF = spark.read.parquet("people.parquet") # Read in the parquet file created above (schema is preserved)

# DataFrame operations
1) def repartition(numPartitions: Int): Dataset[T]
   returns a new DataFrame that has exactly 20 partitions # ex. part-00000, part-00001, ..., part-00020
   ex. dataframe.repartition(20)
2) def write: DataFrameWriter[T]
   interface for saving the content of the non-streaming Dataset out into external storage
   ex. dataframe.repartition(20).write
3) def dataFrameWriter.option(key: String, value: String): DataFrameWriter[T]
   adds an output option for the underlying data source
   ex. dataframe.repartition(20).write.option("sep", "\t")
4) def csv(path: String): Unit
   saves the content of the DataFrame in CSV format at the specified path
   ex. dataframe.repartition(20).write.option("sep", "\t").csv("filepath")
5) def from_json(e: Column, schema: StructType): Column
   import org.apache.spark.sql.functions.from_json
   parses a column containing a JSON string into a StructType with the specified schema (returns null, in the case of an unparseable string)

# DataFrame methods
1) def withColumn(colName: String, col: Column): DataFrame 
   returns a new DataFrame by adding a column or replacing the existing column that has the same name
   ex. val schema = StructType(Seq(StructField("key", StringType, true), StructField("value", DoubleType, true)))
       dataframe.withColumn("jsonData", from_json($"jsonStr", schema))
   ex. val toUpperCaseUDF = udf(
         (str: String) => str.toUpperCase
       )
       dataframe.withColumn("UpperCaseColumn", toUpperCaseUDF(col("col1")))
2) def filter(condition: Column): DataFrame
   filters rows using the given condition
   ex. dataframe.filter(col("age") > 15)
       dataframe.filter(col("name").equalTo("John"))                  // shopping_list
       dataframe.filter(array_contains(col("shopping_list"), "book")) // [tv, book, cd]
   ex. val isHigherUDF = udf(
         (score: Int, threshold: Int) => score > threshold            // (Int, Int) => Boolean
       )
       dataframe.filter(isHigherUDF(col("scores")))

# RDD methods
1) rdd.map([function: Int => Int]) vs. rdd.mapPartitions([function: Iterator[Int] => Iterator[Int]]):
   use of rdd.mapPartitions() to improve the performance
     the map() method maps each element of the rdd to a [function], whereas
     the mapPartitions() method maps each partition's iterator of the rdd to a [function]
   ex. val rdd = sc.parallelize(1 to 9, 3)
       // map():
       def mapDouble(x: Int): (Int, Int) = (x, x * 2)
       val result = rdd.map(mapDouble)
       println(result.collect().mkString) // (1,2)(2,4)(3,6)(4,8)(5,10)(6,12)(7,14)(8,16)(9,18)
   
       // mapPartitions():
       def mapIterDouble(iter: Iterator[Int]): Iterator[(Int, Int)] = {
         var result = List[(Int, Int)]()
         while (iter.hasNext) {
             val x = iter.next
             result .::= (x, x * 2)
         }
         result.iterator
       }
       val result = rdd.mapPartitions(mapIterDouble)
       println(result.collect().mkString) // (1,2)(2,4)(3,6)(4,8)(5,10)(6,12)(7,14)(8,16)(9,18)
2) rdd.groupByKey():
   group the values for each key in the RDD into a single sequence                    //      [key] [value]
   ex. val rdd = sc.makeRDD(Array(("x", 1), ("x", 2), ("y", 1),("y", 2),("z", 1)))    // RDD[(String, Int)]
       rdd.groupByKey                                                                 // RDD[(String, Iterable[Int])]
       rdd.groupByKey.collect  // Array[(String, Iterable[Int])] = Array((z,CompactBuffer(1)), (x,CompactBuffer(1, 2)), (y,CompactBuffer(1, 2)))
3) rdd.reduceByKey([function: (value1, value2) => merged_value]):
   merge the values for each key using an associative and commutative reduce function //      [key] [value]
   ex. val rdd = sc.makeRDD(Array(("x", 1), ("x", 2), ("y", 1),("y", 2),("z", 1)))    // RDD[(String, Int)]
       rdd.reduceByKey                                                                // RDD[(String, Int)] 
       rdd.reduceByKey.collect // Array[(String, Int)] = Array((z,1), (x,3), (y,3)) 
4) rdd.coalesce([numPartitions: Int]) vs. rdd.repartition([numPartitions: Int]):
   coalesce(); return a new RDD that is reduced into numPartitions partitions
   repartition(): coalesce with performing a shuffle 
     i.e. def repartition(numPartitions: Int) = coalesce(numPartitions, shuffle = true)
   note: if you are decreasing the number of partitions in this RDD, consider using coalesce which can avoid performing a shuffle

# debug spark
  the --master option specifies the master URL for a distributed cluster, use local mode for debugging
1) spark-shell --master local[N]: run locally with N threads
   ex. spark-shell --master local[2]
2) spark-shell --master yarn --deploy-mode client: run as client mode
   --master yarn: deploy Spark on top of Hadoop NextGen (YARN)
     this starts a YARN client program which starts the default Application Master
     the application will be run as a child thread of Application Master
     the client will periodically poll the Application Master for status updates and display them in the console, and will exit once 
       the application has finished running
   --deploy-mode client: launch a Spark application in client mode (default: cluster mode)

